# SLM_4_CH
This repository hosts the research and implementation work for a domain-specific Small Language Model (SLM) focused on cultural heritage applications.
The goal is to design, implement, and evaluate lightweight language models that are energy-efficient alternatives to large language models, while maintaining a good accuracy.

## Objectives

- Develop a SLM specific to the Cultural Heritage domain, investigating several architectures and techniques

- Compare energy efficiency, training cost, and accuracy against larger LLM


## Research Questions

- **RQ1**: How much human and computational effort is required to train an SLM from scratch?

- **RQ2**: What is the total cost (in terms of time and energy) of training/fine-tuning an SLM?

- **RQ3**: How do SLMs compare to LLMs in terms of accuracy and energy consumption?
